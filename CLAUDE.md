# HYDRA 10.1 - System Instructions

**Status**: Active | **Mode**: MCP Orchestration | **Project**: ClaudeCLI
**Path**: `C:\Users\BIURODOM\Desktop\ClaudeCLI`
**Config**: `hydra-config.json`

## MCP Tools

| Tool | Port | Funkcja |
|------|------|---------|
| **Serena** | 9000 | Symbolic code analysis |
| **Desktop Commander** | 8100 | System operations |
| **Playwright** | 5200 | Browser automation |

---

## Slash Commands (Quick Reference)

### Core AI Commands

| Command | Description | Cost |
|---------|-------------|------|
| `/ai <query>` | Quick local AI query | $0 |
| `/ai-batch` | Parallel batch queries | $0 |
| `/ai-status` | Check all providers | - |
| `/ai-config` | Configure settings | - |

### Advanced AI Commands

| Command | Description | Module |
|---------|-------------|--------|
| `/self-correct <code task>` | Code with auto-validation | SelfCorrection |
| `/speculate <query>` | Model racing (fastest wins) | SpeculativeDecoding |
| `/semantic-query <file> <question>` | Deep RAG with imports | SemanticFileMapping |
| `/few-shot <task>` | Learn from history | FewShotLearning |
| `/load-balance` | CPU-aware provider | LoadBalancer |
| `/optimize-prompt <text>` | Enhance prompt quality | PromptOptimizer |

### Orchestration

| Command | Description |
|---------|-------------|
| `/hydra` | Three-Headed Beast workflow |
| `/serena-commander` | Serena + DC hybrid skill |

### Usage Examples

```powershell
# Generate validated code
/self-correct Write Python function to parse JSON safely

# Fastest response (model race)
/speculate What is the capital of France?

# Query with full dependency context
/semantic-query src/auth.py How does login work?

# SQL with history examples
/few-shot Write SQL to get active users from last 30 days

# Check system load and get provider
/load-balance

# Improve vague prompt
/optimize-prompt do something with the stuff
```

---

## ðŸ”¥ ZASADA: AI Handler - Auto-Load on Startup

> **AI Handler MUSI byÄ‡ zaÅ‚adowany automatycznie przy kaÅ¼dym starcie ClaudeCLI.**

### Status na starcie

```
  AI Handler:
    Ollama (local)   Running on :11434        [OK]
    Cloud APIs       Anthropic, OpenAI        [OK]
    AI Handler       v1.0 loaded              [OK]
```

### Co jest wÅ‚Ä…czone automatycznie:

| Komponent | Opis | Status |
|-----------|------|--------|
| `AIModelHandler.psm1` | GÅ‚Ã³wny moduÅ‚ | Import globalny |
| `Initialize-AIState` | Stan providerÃ³w | Auto-init |
| Ollama check | Port 11434 | Status w GUI |
| Cloud API keys | Anthropic/OpenAI | Weryfikacja |
| Alias `ai` | Quick queries | Globalny |

### DostÄ™pne komendy po starcie:

```powershell
# Quick AI call (local Ollama preferred)
ai "Twoje pytanie"

# Status wszystkich providerÃ³w
Get-AIStatus

# PeÅ‚ne API call z auto-fallback
Invoke-AIRequest -Messages @(@{role="user"; content="..."})

# Test providerÃ³w
Test-AIProviders
```

### Fallback chain (automatyczny):

```
Local:  Ollama (llama3.2:3b) â†’ qwen2.5-coder:1.5b
Cloud:  Anthropic (Haiku) â†’ OpenAI (gpt-4o-mini)

Priorytet: LOCAL FIRST (koszt $0) â†’ Cloud jako fallback
```

### Implementacja w `_launcher.ps1`:

Sekcja `# === AI HANDLER ===` automatycznie:
1. Importuje moduÅ‚ globalnie
2. Inicjalizuje stan
3. Sprawdza Ollama (local)
4. Weryfikuje klucze API (cloud)
5. Tworzy alias `ai`

**Ta zasada jest OBOWIÄ„ZKOWA** - AI Handler musi byÄ‡ dostÄ™pny natychmiast po starcie bez dodatkowej konfiguracji.

---

## 1. Parallel Execution (Zasada NadrzÄ™dna)

> KaÅ¼da operacja, ktÃ³ra moÅ¼e byÄ‡ wykonana rÃ³wnolegle, MUSI byÄ‡ wykonana rÃ³wnolegle.

### Klasyfikacja

| Typ | Operacje | Wykonanie |
|-----|----------|-----------|
| **READ-ONLY** | `find_symbol`, `read_file`, `list_directory`, `grep`, `glob` | Zawsze rÃ³wnolegle |
| **SIDE-EFFECTS** | `write_file`, `start_process` | Sekwencyjnie |

### Wzorce

```rust
// DOBRZE: tokio::join! dla niezaleÅ¼nych operacji
let (a, b, c) = tokio::join!(task_a(), task_b(), task_c());

// Å¹LE: sekwencyjne await
let a = task_a().await;
let b = task_b().await;  // marnowanie czasu
```

```typescript
// DOBRZE: Promise.all
const [users, products] = await Promise.all([fetchUsers(), fetchProducts()]);

// Å¹LE: await waterfall
const users = await fetchUsers();
const products = await fetchProducts();
```

---

## 2. Council of Six (Multi-Agent Debate)

| Agent | Rola | Fokus |
|-------|------|-------|
| **Architekt** | Fakty | Rust 2024, React 19, czysta struktura |
| **Security** | Ryzyko | ENV vars allowed, zero commits wraÅ¼liwych danych, maskowanie kluczy API |
| **Speedster** | Performance | Lighthouse > 90, bundle < 200KB |
| **Pragmatyk** | KorzyÅ›ci | HybrydowoÅ›Ä‡ Web + Desktop |
| **Researcher** | Weryfikacja | Sprawdzaj w docs/Google przed implementacjÄ… |
| **Jester** | Emocje | Krytyka boilerplate'u i over-engineeringu |

---

## 3. Tech Stack (ClaudeCLI)

| Warstwa | Technologia |
|---------|-------------|
| **Shell** | PowerShell 7 |
| **MCP Servers** | Serena, Desktop Commander, Playwright |
| **Config** | JSON, YAML, Markdown |
| **OS** | Windows 11 |

---

## 4. Project Structure

```
C:\Users\BIURODOM\Desktop\ClaudeCLI\
â”œâ”€â”€ .claude/
â”‚   â”œâ”€â”€ commands/        # Custom slash commands
â”‚   â”œâ”€â”€ hooks/           # Event hooks
â”‚   â”œâ”€â”€ skills/          # Custom skills (serena-commander, hydra)
â”‚   â”œâ”€â”€ settings.local.json
â”‚   â””â”€â”€ statusline.js    # Status bar config
â”œâ”€â”€ .serena/
â”‚   â”œâ”€â”€ cache/           # Serena cache
â”‚   â”œâ”€â”€ memories/        # Persistent memories (25 slots)
â”‚   â””â”€â”€ project.yml      # Serena project config
â”œâ”€â”€ .gitignore           # Ochrona sekretÃ³w
â”œâ”€â”€ CLAUDE.md            # Ten plik (instrukcje)
â”œâ”€â”€ _launcher.ps1        # Main launcher
â”œâ”€â”€ mcp-health-check.ps1 # MCP diagnostics
â””â”€â”€ ClaudeCLI.vbs        # Windows shortcut helper
â”œâ”€â”€ ai-handler/          # ðŸ¤– AI Model Handler with auto-fallback
â”‚   â”œâ”€â”€ AIFacade.psm1            # ðŸŽ¯ ENTRY POINT - Unified interface (NEW)
â”‚   â”œâ”€â”€ AIModelHandler.psm1      # Legacy main module (still works)
â”‚   â”œâ”€â”€ ai-config.json           # Provider/model configuration
â”‚   â”œâ”€â”€ ai-state.json            # Runtime state (auto-generated)
â”‚   â”œâ”€â”€ Invoke-AI.ps1            # Quick CLI wrapper
â”‚   â”œâ”€â”€ Initialize-AIHandler.ps1 # Setup script
â”‚   â”œâ”€â”€ Initialize-AdvancedAI.ps1 # Advanced AI loader
â”‚   â”œâ”€â”€ Demo-AdvancedAI.ps1      # Advanced features demo
â”‚   â”œâ”€â”€ cache/                   # Few-shot learning cache
â”‚   â”œâ”€â”€ utils/                   # ðŸ“¦ Layer 1: Utilities (NEW)
â”‚   â”‚   â”œâ”€â”€ AIUtil-JsonIO.psm1       # Atomic JSON read/write
â”‚   â”‚   â”œâ”€â”€ AIUtil-Health.psm1       # System & provider health checks
â”‚   â”‚   â”œâ”€â”€ AIUtil-Validation.psm1   # Prompt/code validation
â”‚   â”‚   â””â”€â”€ AIErrorHandler.psm1      # Centralized error handling
â”‚   â”œâ”€â”€ core/                    # ðŸ“¦ Layer 2: Core (NEW)
â”‚   â”‚   â”œâ”€â”€ AIConstants.psm1         # System constants
â”‚   â”‚   â”œâ”€â”€ AIConfig.psm1            # Configuration management
â”‚   â”‚   â””â”€â”€ AIState.psm1             # Runtime state management
â”‚   â”œâ”€â”€ rate-limiting/           # ðŸ“¦ Layer 3: Rate limiting (NEW)
â”‚   â”‚   â””â”€â”€ RateLimiter.psm1         # Token/request rate limiting
â”‚   â”œâ”€â”€ model-selection/         # ðŸ“¦ Layer 3: Model selection (NEW)
â”‚   â”‚   â””â”€â”€ ModelSelector.psm1       # Intelligent model selection
â”‚   â”œâ”€â”€ providers/               # ðŸ“¦ Layer 4: Providers (NEW)
â”‚   â”‚   â”œâ”€â”€ OllamaProvider.psm1      # Local Ollama integration
â”‚   â”‚   â”œâ”€â”€ AnthropicProvider.psm1   # Anthropic Claude API
â”‚   â”‚   â””â”€â”€ OpenAIProvider.psm1      # OpenAI GPT API
â”‚   â””â”€â”€ modules/                 # ðŸ§  Advanced AI Modules (Layer 6)
â”‚       â”œâ”€â”€ SelfCorrection.psm1      # Agentic self-correction
â”‚       â”œâ”€â”€ FewShotLearning.psm1     # Dynamic few-shot learning
â”‚       â”œâ”€â”€ SpeculativeDecoding.psm1 # Parallel multi-model
â”‚       â”œâ”€â”€ LoadBalancer.psm1        # CPU-aware load balancing
â”‚       â”œâ”€â”€ SemanticFileMapping.psm1 # Deep RAG with imports
â”‚       â”œâ”€â”€ AdvancedAI.psm1          # Unified interface
â”‚       â”œâ”€â”€ PromptOptimizer.psm1     # Auto prompt enhancement
â”‚       â”œâ”€â”€ TaskClassifier.psm1      # Task type classification
â”‚       â”œâ”€â”€ SmartQueue.psm1          # Prompt queue management
â”‚       â”œâ”€â”€ ModelDiscovery.psm1      # Dynamic model discovery
â”‚       â”œâ”€â”€ SemanticGitCommit.psm1   # AI-powered git commits
â”‚       â”œâ”€â”€ AICodeReview.psm1        # Code review module
â”‚       â””â”€â”€ PredictiveAutocomplete.psm1 # Autocomplete suggestions
â”œâ”€â”€ parallel/            # âš¡ Parallel execution system
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â””â”€â”€ ParallelUtils.psm1    # Core parallel functions
â”‚   â”œâ”€â”€ build/
â”‚   â”‚   â”œâ”€â”€ Build-Parallel.ps1    # Multi-project builder
â”‚   â”‚   â”œâ”€â”€ Test-Parallel.ps1     # Parallel test runner
â”‚   â”‚   â””â”€â”€ Lint-Parallel.ps1     # Parallel linter
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ Invoke-ParallelGit.ps1       # Git ops across repos
â”‚   â”‚   â”œâ”€â”€ Invoke-ParallelDownload.ps1  # Multi-connection downloads
â”‚   â”‚   â”œâ”€â”€ Invoke-ParallelCompress.ps1  # 7z parallel compression
â”‚   â”‚   â”œâ”€â”€ Invoke-TaskDAG.ps1           # Dependency-aware executor
â”‚   â”‚   â”œâ”€â”€ Watch-FilesParallel.ps1      # File system watcher
â”‚   â”‚   â”œâ”€â”€ Invoke-MCPParallel.ps1       # MCP parallelization guide
â”‚   â”‚   â””â”€â”€ Start-ParallelBrowsers.ps1   # Playwright parallel helper
â”‚   â””â”€â”€ Initialize-Parallel.ps1   # Module loader
```

---

## 4.1 Refactored AI Handler Architecture (ðŸ—ï¸ NEW)

The AI Handler system has been refactored into a modular, layered architecture with dependency injection and proper separation of concerns. This replaces the monolithic `AIModelHandler.psm1` design.

### New Directory Structure

```
ai-handler/
â”œâ”€â”€ AIFacade.psm1              # ðŸŽ¯ ENTRY POINT - Unified interface
â”œâ”€â”€ AIModelHandler.psm1        # Legacy module (still works)
â”œâ”€â”€ ai-config.json             # Provider/model configuration
â”œâ”€â”€ ai-state.json              # Runtime state (auto-generated)
â”‚
â”œâ”€â”€ utils/                     # ðŸ“¦ Layer 1: Utilities (no dependencies)
â”‚   â”œâ”€â”€ AIUtil-JsonIO.psm1     # Atomic JSON read/write
â”‚   â”œâ”€â”€ AIUtil-Health.psm1     # System & provider health checks
â”‚   â”œâ”€â”€ AIUtil-Validation.psm1 # Prompt/code validation
â”‚   â””â”€â”€ AIErrorHandler.psm1    # Centralized error handling
â”‚
â”œâ”€â”€ core/                      # ðŸ“¦ Layer 2: Core (depends on utils)
â”‚   â”œâ”€â”€ AIConstants.psm1       # System constants
â”‚   â”œâ”€â”€ AIConfig.psm1          # Configuration management
â”‚   â””â”€â”€ AIState.psm1           # Runtime state management
â”‚
â”œâ”€â”€ rate-limiting/             # ðŸ“¦ Layer 3: Infrastructure
â”‚   â””â”€â”€ RateLimiter.psm1       # Token/request rate limiting
â”‚
â”œâ”€â”€ model-selection/           # ðŸ“¦ Layer 3: Infrastructure
â”‚   â””â”€â”€ ModelSelector.psm1     # Optimal model selection
â”‚
â”œâ”€â”€ providers/                 # ðŸ“¦ Layer 4: Providers
â”‚   â”œâ”€â”€ OllamaProvider.psm1    # Local Ollama integration
â”‚   â”œâ”€â”€ AnthropicProvider.psm1 # Anthropic Claude API
â”‚   â””â”€â”€ OpenAIProvider.psm1    # OpenAI GPT API
â”‚
â”œâ”€â”€ fallback/                  # ðŸ“¦ Layer 5: Fallback logic
â”‚   â””â”€â”€ (fallback orchestration)
â”‚
â””â”€â”€ modules/                   # ðŸ“¦ Layer 6: Advanced features
    â”œâ”€â”€ SelfCorrection.psm1
    â”œâ”€â”€ FewShotLearning.psm1
    â”œâ”€â”€ SpeculativeDecoding.psm1
    â”œâ”€â”€ LoadBalancer.psm1
    â”œâ”€â”€ SemanticFileMapping.psm1
    â”œâ”€â”€ PromptOptimizer.psm1
    â”œâ”€â”€ AdvancedAI.psm1
    â””â”€â”€ ... (other advanced modules)
```

### Layer Descriptions

| Layer | Directory | Responsibility | Dependencies |
|-------|-----------|----------------|--------------|
| **1. Utils** | `utils/` | Zero-dependency utilities | None |
| **2. Core** | `core/` | Configuration, constants, state | Utils |
| **3. Infrastructure** | `rate-limiting/`, `model-selection/` | Rate limiting, model selection | Core |
| **4. Providers** | `providers/` | API integrations | Infrastructure |
| **5. Fallback** | `fallback/` | Cross-provider fallback | Providers |
| **6. Advanced** | `modules/` | Optional advanced features | All above |

### AIFacade.psm1 - The Entry Point

`AIFacade.psm1` is the **recommended entry point** for all AI operations. It provides:

1. **Dependency Injection Container** - Manages module loading order
2. **Phased Loading** - Prevents circular dependencies
3. **Unified Interface** - Single `Invoke-AI` function for all operations

```powershell
# Initialize the AI System (loads all modules in correct order)
Import-Module "C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler\AIFacade.psm1"
$result = Initialize-AISystem

# Check system status
Get-AISystemStatus -Detailed -CheckProviders

# Unified AI invocation
Invoke-AI "What is 2+2?" -Mode fast
Invoke-AI "Write Python function to sort list" -Mode code
Invoke-AI "Explain async/await" -Mode analysis

# Get dependency container
Get-AIDependencies -Category "Providers"

# Reset and reinitialize
Reset-AISystem -Reinitialize
```

### Key Module Responsibilities

| Module | Functions | Description |
|--------|-----------|-------------|
| **AIUtil-JsonIO** | `Read-JsonFile`, `Write-JsonFile`, `ConvertTo-Hashtable` | Atomic JSON I/O with PS 5.1 compatibility |
| **AIUtil-Health** | `Test-OllamaAvailable`, `Get-SystemMetrics`, `Test-ProviderConnectivity` | Cached health checks (30s TTL) |
| **AIUtil-Validation** | `Get-PromptCategory`, `Get-ClarityScore`, `Test-CodeLanguage` | Prompt/code analysis |
| **AIErrorHandler** | `Get-ErrorCategory`, `Test-ErrorRecoverable`, `Get-RetryStrategy` | Error classification & recovery |
| **AIConfig** | `Get-AIConfig`, `Save-AIConfig`, `Merge-Config`, `Test-ConfigValid` | Configuration CRUD |
| **AIState** | `Get-AIState`, `Save-AIState`, `Update-AIState` | Runtime state management |
| **RateLimiter** | `Update-UsageTracking`, `Get-RateLimitStatus`, `Test-RateLimitAvailable` | Per-minute rate limiting |
| **ModelSelector** | `Get-OptimalModel`, `Get-FallbackModel`, `Test-ModelAvailable` | Intelligent model selection |
| **OllamaProvider** | `Test-OllamaAvailable`, `Get-OllamaModels`, `Invoke-OllamaAPI` | Local AI via Ollama |
| **AnthropicProvider** | `Invoke-AnthropicAPI`, `Test-AnthropicAvailable` | Claude API integration |
| **OpenAIProvider** | `Invoke-OpenAIAPI`, `Test-OpenAIAvailable` | GPT API integration |

### Migration Guide: Old vs New

**OLD (Monolithic)**:
```powershell
# Single file import
Import-Module "ai-handler\AIModelHandler.psm1"
Invoke-AIRequest -Messages @(@{role="user"; content="..."})
```

**NEW (Modular via Facade)**:
```powershell
# Facade handles all dependencies
Import-Module "ai-handler\AIFacade.psm1"
Initialize-AISystem

# Unified interface with mode selection
Invoke-AI "Your prompt" -Mode auto

# Or use individual modules if needed
$status = Get-RateLimitStatus -Provider "anthropic" -Model "claude-sonnet-4-5-20250929"
$optimal = Get-OptimalModel -Task "code" -PreferCheapest
```

**Backward Compatibility**: `AIModelHandler.psm1` still works as before. The new architecture is additive.

### Loading Phases (Initialize-AISystem)

```
Phase 1: Utils          â†’ AIUtil-JsonIO, AIUtil-Health, AIUtil-Validation
Phase 2: Core           â†’ AIConstants, AIConfig, AIState
Phase 3: Infrastructure â†’ RateLimiter, ModelSelector, ErrorLogger, SecureStorage
Phase 4: Providers      â†’ OllamaProvider, AnthropicProvider, OpenAIProvider
Phase 5: Advanced       â†’ SelfCorrection, FewShotLearning, SpeculativeDecoding, ...
```

### Utility Functions Quick Reference

#### AIUtil-JsonIO (Atomic JSON Operations)

```powershell
# Read JSON with default fallback
$config = Read-JsonFile -Path "config.json" -Default @{ setting = "value" }

# Write JSON atomically (temp file + rename)
Write-JsonFile -Path "state.json" -Data $state -Depth 10

# Convert PSObject to Hashtable (PS 5.1 compatibility)
$hashtable = $jsonObject | ConvertTo-Hashtable
```

#### AIUtil-Health (Cached Health Checks)

```powershell
# Check Ollama (cached 30s)
$ollama = Test-OllamaAvailable -IncludeModels
# Returns: @{ Available = $true; Models = @('llama3.2:3b'); ResponseTimeMs = 15 }

# Force fresh check
Test-OllamaAvailable -NoCache

# Get system metrics (cached 10s)
$metrics = Get-SystemMetrics
# Returns: @{ CpuPercent = 25; MemoryPercent = 60; ... }
```

#### AIUtil-Validation (Prompt Analysis)

```powershell
# Detect prompt category
Get-PromptCategory -Prompt "Write Python function to sort"
# Returns: "code"

# Get clarity score (0-100)
Get-ClarityScore -Prompt "do something with the stuff"
# Returns: 35 (low - vague terms detected)

# Detect programming language in code
Test-CodeLanguage -Code "def hello(): print('world')"
# Returns: "python"
```

#### AIErrorHandler (Error Classification)

```powershell
# Classify error
$category = Get-ErrorCategory -ErrorMessage "rate limit exceeded"
# Returns: "RateLimit"

# Check if recoverable
Test-ErrorRecoverable -Category "RateLimit"
# Returns: $true

# Get retry strategy
Get-RetryStrategy -Category "Overloaded"
# Returns: @{ RetryAfter = 30000; Fallback = "SwitchModel" }
```

### Benefits of New Architecture

| Benefit | Description |
|---------|-------------|
| **No Circular Dependencies** | Phased loading ensures correct order |
| **Testability** | Individual modules can be unit tested |
| **Maintainability** | Single responsibility per module |
| **Caching** | Health checks cached to reduce redundancy |
| **Backward Compatible** | Old code still works unchanged |
| **Dependency Injection** | Easy to swap implementations |
| **Error Isolation** | Failures in optional modules don't break core |

---

## 5. Parallel Execution System (âš¡ NEW)

### Quick Start

```powershell
# Initialize parallel environment
. "C:\Users\BIURODOM\Desktop\ClaudeCLI\parallel\Initialize-Parallel.ps1"

# Check system configuration
Get-ParallelConfig
```

### Module Functions

| Function | Description | Usage |
|----------|-------------|-------|
| `Invoke-Parallel` | General parallel execution | `$items \| Invoke-Parallel { process $_ }` |
| `Invoke-ParallelJobs` | Run multiple jobs | `Invoke-ParallelJobs -Jobs @{...} -Wait` |
| `Read-FilesParallel` | Read multiple files | `Read-FilesParallel -Paths @(...)` |
| `Search-FilesParallel` | Search across dirs | `Search-FilesParallel -Paths @(...) -Pattern "TODO"` |
| `Invoke-CommandsParallel` | Run shell commands | `Invoke-CommandsParallel -Commands @(...)` |
| `Invoke-WebRequestsParallel` | HTTP requests | `Invoke-WebRequestsParallel -Urls @(...)` |
| `Invoke-GitParallel` | Git across repos | `Invoke-GitParallel -Repositories @(...) -GitCommand "pull"` |
| `Compress-FilesParallel` | 7z compression | `Compress-FilesParallel -Items @(...)` |

### Build Scripts

```powershell
# Build all projects (Node, Rust, .NET, Python, Go)
& "parallel\build\Build-Parallel.ps1" -Path "C:\Projects" -Test -Clean

# Run tests with coverage
& "parallel\build\Test-Parallel.ps1" -Path "C:\Projects" -Coverage

# Lint and auto-fix
& "parallel\build\Lint-Parallel.ps1" -Path "C:\Projects" -Fix
```

### Task DAG (Dependency-Aware Execution)

```powershell
$tasks = @{
    "install" = @{ Script = { npm install }; DependsOn = @() }
    "build" = @{ Script = { npm run build }; DependsOn = @("install") }
    "test" = @{ Script = { npm test }; DependsOn = @("build") }
    "lint" = @{ Script = { npm run lint }; DependsOn = @("install") }
    "deploy" = @{ Script = { npm run deploy }; DependsOn = @("test", "lint") }
}

& "parallel\scripts\Invoke-TaskDAG.ps1" -Tasks $tasks
```

### MCP Parallelization Rules

| Operation Type | Parallelization | Example |
|---------------|-----------------|---------|
| **READ-ONLY** | Always parallel | `read_file`, `list_directory`, `find_symbol`, `start_search` |
| **WRITE** | Sequential | `write_file`, `edit_block` |
| **BROWSER** | Multi-tab parallel | Multiple `browser_navigate` calls |

**Claude MUST batch independent MCP calls in single message:**
```
âœ… GOOD: [read_file: a.txt] [read_file: b.txt] [find_symbol: MyClass]
âŒ BAD:  Message 1: [read_file: a.txt]
        Message 2: [read_file: b.txt]
        Message 3: [find_symbol: MyClass]
```

### Performance Targets

| Metric | Target |
|--------|--------|
| CPU Utilization | > 80% during parallel ops |
| Speedup (N cores) | > N/2 for I/O bound tasks |
| Build time | < sequential / core_count |

---

## 6. AI Model Handler (ðŸ¤– NEW)

Comprehensive AI model management with automatic fallback, rate limiting, cost optimization, and multi-provider support.

### Features

| Feature | Description |
|---------|-------------|
| **Auto-Retry Fallback** | Opus â†’ Sonnet â†’ Haiku on errors |
| **Rate Limit Aware** | Auto-switch when approaching limits |
| **Cost Optimizer** | Select cheapest model for task |
| **Multi-Provider** | Anthropic â†’ OpenAI â†’ Ollama fallback |

### Quick Start

```powershell
# Initialize (run once per session)
. "C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler\Initialize-AIHandler.ps1"

# Quick AI call
.\ai-handler\Invoke-AI.ps1 -Prompt "Your question"

# With task-specific optimization
.\ai-handler\Invoke-AI.ps1 -Prompt "Write Python code" -Task code -PreferCheapest

# Check status
Get-AIStatus

# Test all providers
Test-AIProviders
```

### Model Configuration

| Provider | Models | Pricing (per 1M tokens) |
|----------|--------|------------------------|
| **Anthropic** | Opus 4.5, Sonnet 4.5, Haiku 4 | $15-$0.80 in / $75-$4 out |
| **OpenAI** | GPT-4o, GPT-4o-mini | $2.50-$0.15 in / $10-$0.60 out |
| **Ollama** | Llama 3.3, Qwen 2.5 | Free (local) |

### Fallback Chain

```
Anthropic: Opus 4.5 â†’ Sonnet 4.5 â†’ Haiku 4
OpenAI:    GPT-4o â†’ GPT-4o-mini
Ollama:    Llama 3.3:70b â†’ Qwen 2.5-coder:32b

Provider Order: Anthropic â†’ OpenAI â†’ Ollama
```

### Module Functions

| Function | Description |
|----------|-------------|
| `Get-AIStatus` | View all providers and rate limits |
| `Test-AIProviders` | Test connectivity to all providers |
| `Get-OptimalModel` | Auto-select best model for task type |
| `Get-FallbackModel` | Get next model in fallback chain |
| `Invoke-AIRequest` | Make AI request with auto-fallback |
| `Update-UsageTracking` | Log usage for rate limiting |
| `Reset-AIState` | Clear usage data |

### Task-Based Selection

```powershell
# Get optimal model for task type
Get-OptimalModel -Task "code" -EstimatedTokens 1000 -PreferCheapest

# Task types: simple, complex, creative, code, vision, analysis
```

### Rate Limit Monitoring

```powershell
# Check rate limit status
Get-RateLimitStatus -Provider "anthropic" -Model "claude-sonnet-4-5-20250929"

# Returns: available, tokensPercent, requestsPercent, tokensRemaining
```

### Configuration

Edit `ai-handler/ai-config.json`:

```json
{
  "settings": {
    "maxRetries": 3,
    "retryDelayMs": 1000,
    "rateLimitThreshold": 0.85,
    "costOptimization": true,
    "autoFallback": true
  }
}
```

### Environment Variables

| Variable | Provider | Required |
|----------|----------|----------|
| `ANTHROPIC_API_KEY` | Anthropic | For Claude models |
| `OPENAI_API_KEY` | OpenAI | For GPT models |
| (none) | Ollama | Local, no key needed |

---

## 7. Security Policy

### Environment Variables Access

ClaudeCLI ma dostÄ™p do zmiennych Å›rodowiskowych systemu operacyjnego.

#### Dozwolone operacje:

| Operacja | Opis | PrzykÅ‚ad |
|----------|------|----------|
| **Odczyt** | PeÅ‚ny dostÄ™p do wszystkich zmiennych Å›rodowiskowych | `$env:ANTHROPIC_API_KEY`, `$env:PATH` |
| **WyÅ›wietlanie** | Lista i podglÄ…d wartoÅ›ci (maskowanie kluczy API) | `Get-ChildItem env:` |
| **Weryfikacja** | Sprawdzanie obecnoÅ›ci i formatowania | `if ($env:VAR) { }` |

#### Zabezpieczenia:

```powershell
# âœ… DOZWOLONE: Odczyt zmiennych Å›rodowiskowych
$apiKey = $env:ANTHROPIC_API_KEY
$path = $env:PATH
$user = $env:USERNAME

# âœ… DOZWOLONE: Maskowanie wraÅ¼liwych danych w outputach
if ($apiKey) {
    Write-Host "API Key: $($apiKey.Substring(0, 15))..." -ForegroundColor Green
}

# âŒ ZABRONIONE: Hardcoding sekretÃ³w w kodzie
$apiKey = "sk-ant-api03-hardcoded..."  # NIGDY!

# âŒ ZABRONIONE: Commitowanie kluczy do Git
# UÅ¼ywaj .gitignore dla plikÃ³w z sekretami
```

#### Konfiguracja API Keys:

| Zmienna Å›rodowiskowa | Provider | Wymagane |
|---------------------|----------|----------|
| `ANTHROPIC_API_KEY` | Anthropic Claude | âœ… Tak |

**Jak ustawiÄ‡:**

```powershell
# PowerShell (User scope - rekomendowane)
[Environment]::SetEnvironmentVariable('ANTHROPIC_API_KEY', 'sk-ant-api03-...', 'User')

# PowerShell (Process scope - tymczasowe)
$env:ANTHROPIC_API_KEY = "sk-ant-api03-..."

# CMD
setx ANTHROPIC_API_KEY "sk-ant-api03-..."
```

**Weryfikacja:**

```powershell
# Lista zmiennych Claude/Anthropic
Get-ChildItem env: | Where-Object { $_.Name -like "*CLAUDE*" -or $_.Name -like "*ANTHROPIC*" }

# SprawdÅº konkretnÄ… zmiennÄ… (maskowana)
$key = $env:ANTHROPIC_API_KEY
if ($key) { Write-Host "âœ“ API Key: $($key.Substring(0,15))..." }
```

### Permissions Model

ClaudeCLI uÅ¼ywa modelu uprawnieÅ„ zdefiniowanego w `.claude/settings.local.json`:

```json
{
  "permissions": {
    "allow": [
      "Bash",
      "mcp__serena__*",
      "mcp__desktop-commander__*",
      "mcp__playwright__*",
      "Write",
      "Edit",
      "Read",
      "Glob",
      "Grep"
    ],
    "deny": []
  }
}
```

**Rozszerzone uprawnienia (wildcard `*`)**: ClaudeCLI ma peÅ‚ny dostÄ™p do wszystkich operacji Bash, MCP i narzÄ™dzi do zarzÄ…dzania plikami.

### Maximum Autonomy Mode

ClaudeCLI dziaÅ‚a w trybie maksymalnej autonomii z peÅ‚nymi uprawnieniami systemowymi.

#### ðŸ”“ Rozszerzone uprawnienia Å›rodowiskowe:

| Operacja | Opis | PrzykÅ‚ad |
|----------|------|----------|
| **Modyfikacja ENV** | Ustawianie i zmiana zmiennych Å›rodowiskowych | `[Environment]::SetEnvironmentVariable()` |
| **Usuwanie ENV** | Kasowanie zmiennych Å›rodowiskowych | `Remove-Item Env:\VAR` |
| **Scope control** | User, Machine, Process scopes | `-Scope Machine` |

```powershell
# âœ… Modyfikacja zmiennych Å›rodowiskowych
[Environment]::SetEnvironmentVariable('NEW_VAR', 'value', 'User')
[Environment]::SetEnvironmentVariable('SYSTEM_VAR', 'value', 'Machine')

# âœ… Usuwanie
[Environment]::SetEnvironmentVariable('OLD_VAR', $null, 'User')
Remove-Item Env:\TEMP_VAR
```

#### ðŸ”§ DostÄ™p do Rejestru Windows:

```powershell
# âœ… Odczyt rejestru
Get-ItemProperty -Path "HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion"

# âœ… Modyfikacja rejestru
New-ItemProperty -Path "HKCU:\Software\MyApp" -Name "Setting" -Value "1"
Set-ItemProperty -Path "HKCU:\Software\MyApp" -Name "Setting" -Value "2"

# âœ… Tworzenie kluczy
New-Item -Path "HKCU:\Software\MyApp\Config"
```

#### ðŸ” Uprawnienia Administracyjne:

```powershell
# âœ… Sprawdzanie uprawnieÅ„ admin
$isAdmin = ([Security.Principal.WindowsPrincipal][Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole]::Administrator)

# âœ… Uruchamianie jako Administrator
Start-Process powershell -Verb RunAs -ArgumentList "-Command", "Write-Host 'Admin mode'"

# âœ… ZarzÄ…dzanie usÅ‚ugami systemowymi
Get-Service | Where-Object {$_.Status -eq 'Running'}
Start-Service -Name "ServiceName"
Stop-Service -Name "ServiceName" -Force
Restart-Service -Name "ServiceName"
```

#### ðŸ“‚ PeÅ‚ny dostÄ™p do systemu plikÃ³w:

```powershell
# âœ… DostÄ™p do katalogÃ³w systemowych
Get-ChildItem "C:\Windows\System32"
Get-ChildItem "C:\Program Files"
Get-ChildItem "$env:APPDATA"

# âœ… Modyfikacja plikÃ³w systemowych (ostroÅ¼nie!)
Copy-Item "C:\file.txt" "C:\Windows\System32\file.txt" -Force

# âœ… Operacje masowe
Get-ChildItem -Path "C:\Temp" -Recurse | Remove-Item -Force -Recurse
```

#### ðŸŒ Operacje sieciowe:

```powershell
# âœ… Skanowanie portÃ³w
Test-NetConnection -ComputerName "example.com" -Port 80

# âœ… Pobieranie plikÃ³w
Invoke-WebRequest -Uri "https://example.com/file.zip" -OutFile "C:\file.zip"
curl -O "https://example.com/file.zip"
wget "https://example.com/file.zip"

# âœ… Konfiguracja firewall
New-NetFirewallRule -DisplayName "Allow Port 8080" -Direction Inbound -LocalPort 8080 -Protocol TCP -Action Allow
```

#### ðŸ“¦ Instalacja oprogramowania:

```powershell
# âœ… Chocolatey
choco install nodejs -y
choco upgrade all -y

# âœ… Winget
winget install Microsoft.VisualStudioCode
winget upgrade --all

# âœ… NPM/Yarn/PNPM global packages
npm install -g typescript
pnpm add -g next

# âœ… Python pip global
pip install --upgrade pip
pip install requests pandas numpy
```

#### âš¡ Wykonywanie skryptÃ³w bez ograniczeÅ„:

```powershell
# âœ… Zmiana Execution Policy
Set-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser

# âœ… Uruchamianie skryptÃ³w z internetu
Invoke-Expression (Invoke-WebRequest -Uri "https://example.com/script.ps1").Content

# âœ… Wykonywanie kodu z plikÃ³w
. "C:\Scripts\MyScript.ps1"
& "C:\Scripts\MyScript.ps1"

# âœ… Background jobs
Start-Job -ScriptBlock { Get-Process | Export-Csv "processes.csv" }
```

#### ðŸ–¥ï¸ ZarzÄ…dzanie procesami:

```powershell
# âœ… Lista procesÃ³w
Get-Process | Sort-Object CPU -Descending | Select-Object -First 10

# âœ… Zabijanie procesÃ³w
Stop-Process -Name "chrome" -Force
Get-Process | Where-Object {$_.CPU -gt 100} | Stop-Process -Force

# âœ… Uruchamianie z priorytetem
Start-Process notepad.exe -WindowStyle Maximized
```

#### ðŸ”„ Automatyzacja zadaÅ„:

```powershell
# âœ… Task Scheduler
$action = New-ScheduledTaskAction -Execute "PowerShell.exe" -Argument "-File C:\script.ps1"
$trigger = New-ScheduledTaskTrigger -Daily -At 9am
Register-ScheduledTask -Action $action -Trigger $trigger -TaskName "MyTask"

# âœ… Monitorowanie zmian w plikach
$watcher = New-Object System.IO.FileSystemWatcher
$watcher.Path = "C:\Watch"
$watcher.EnableRaisingEvents = $true
```

### âš ï¸ OstrzeÅ¼enia

Mimo maksymalnych uprawnieÅ„, zalecane jest:
- âœ… Tworzenie backupÃ³w przed modyfikacjami systemowymi
- âœ… Testowanie na Å›rodowisku deweloperskim
- âœ… Logowanie wszystkich operacji systemowych
- âŒ Unikanie nieodwracalnych operacji bez potwierdzenia

---

## 8. Protocols

### PowerShell
- **Error handling**: `try/catch` z `-ErrorAction Stop`
- **Logging**: Write-Host z kolorami dla statusÃ³w
- **Paths**: Zawsze absolutne Å›cieÅ¼ki Windows

### MCP
- **Health check**: `mcp-health-check.ps1` przed startem
- **Ports**: Desktop Commander (8100), Playwright (5200)
- **Memories**: Serena max 25 slotÃ³w w `.serena/memories/`

---

## 9. Best Practices (Zalecenia w trybie Maximum Autonomy)

W trybie maksymalnej autonomii masz peÅ‚nÄ… swobodÄ™, ale przestrzegaj dobrych praktyk:

| Zalecenie | PowÃ³d | Priorytet |
|-----------|-------|-----------|
| UÅ¼ywaj ENV vars zamiast hardcoded keys | Security - Å‚atwiejsze zarzÄ…dzanie | ðŸ”´ Krytyczny |
| NIE commituj kluczy API do Git | Publiczne wycieki - uÅ¼ywaj .gitignore | ðŸ”´ Krytyczny |
| Maskuj klucze API w outputach | Security - pokaÅ¼ tylko pierwszych 15 znakÃ³w | ðŸŸ¡ Åšredni |
| Preferuj absolute paths | BÅ‚Ä™dy na rÃ³Å¼nych maszynach | ðŸŸ¢ Niski |
| Zawsze uÅ¼ywaj error handling | Ciche faile sÄ… gorsze niÅ¼ crashe | ðŸŸ¡ Åšredni |
| Parallel MCP calls gdy moÅ¼liwe | Performance - uÅ¼ywaj tokio::join! / Promise.all | ðŸŸ¡ Åšredni |
| Health check przed MCP operations | MCP moÅ¼e byÄ‡ down - sprawdÅº przed uÅ¼yciem | ðŸŸ¢ Niski |
| Backupy przed systemowymi zmianami | MoÅ¼liwoÅ›Ä‡ rollbacku | ðŸŸ¡ Åšredni |
| Test na dev przed produkcjÄ… | UnikniÄ™cie nieodwracalnych bÅ‚Ä™dÃ³w | ðŸŸ¡ Åšredni |

### ðŸš¨ Absolutne zakazy (nawet w Maximum Autonomy):

| Zakaz | PowÃ³d |
|-------|-------|
| `rm -rf /` lub `Remove-Item C:\ -Recurse -Force` | Zniszczenie systemu |
| `format C:` | Formatowanie dysku systemowego |
| `diskpart` bez potwierdzenia | Nieodwracalne zmiany partycji |
| Masowe usuwanie kluczy rejestru | Destabilizacja systemu |
| WyÅ‚Ä…czanie Windows Defender bez zgody | ZagroÅ¼enie bezpieczeÅ„stwa |

**Filozofia**: Masz peÅ‚nÄ… moc, ale z wielkÄ… mocÄ… idzie wielka odpowiedzialnoÅ›Ä‡. Przed destrukcyjnymi operacjami - ASK USER!

---

## 10. AI Handler - Matryca Decyzyjna

### Kiedy uÅ¼ywaÄ‡ AI Handler?

| Scenariusz | Decyzja | Provider | Model | Metoda |
|------------|---------|----------|-------|--------|
| **Proste pytanie** (1 prompt) | Local | ollama | llama3.2:3b | `Invoke-AIRequest` |
| **Batch processing** (wiele promptÃ³w) | Local + Parallel | ollama | llama3.2:3b | `Invoke-AIBatch` |
| **Generowanie kodu** | Local (code-specific) | ollama | qwen2.5-coder:1.5b | `Invoke-AIRequest` |
| **Szybka odpowiedÅº** (niski latency) | Local (smallest) | ollama | llama3.2:1b | `Invoke-AIRequest` |
| **ZÅ‚oÅ¼one zadanie** (wymaga reasoning) | Cloud fallback | anthropic | claude-3-5-haiku | `Invoke-AIRequest -AutoFallback` |
| **Krytyczne zadanie** (najwyÅ¼sza jakoÅ›Ä‡) | Cloud | anthropic | claude-sonnet-4 | `Invoke-AIRequest -Provider anthropic` |

### Automatyczny wybÃ³r (DOMYÅšLNY)

```powershell
# Import moduÅ‚u
Import-Module "C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler\AIModelHandler.psm1"

# Automatycznie wybierze lokalny Ollama (preferLocal = true)
$response = Invoke-AIRequest -Messages @(@{role="user"; content="..."})
```

### ÅšcieÅ¼ka decyzyjna

```
START
  â”‚
  â”œâ”€ Czy zadanie wymaga wielu promptÃ³w? â”€â”€â”€ TAK â”€â”€â†’ Invoke-AIBatch (parallel)
  â”‚                                                    â”‚
  â”‚                                                    â””â”€â†’ ollama/llama3.2:3b (4 concurrent)
  â”‚
  â””â”€ NIE (single prompt)
       â”‚
       â”œâ”€ Czy to generowanie kodu? â”€â”€â”€ TAK â”€â”€â†’ ollama/qwen2.5-coder:1.5b
       â”‚
       â”œâ”€ Czy potrzebujÄ™ szybkiej odpowiedzi? â”€â”€â”€ TAK â”€â”€â†’ ollama/llama3.2:1b
       â”‚
       â”œâ”€ Czy to zÅ‚oÅ¼one reasoning? â”€â”€â”€ TAK â”€â”€â†’ anthropic/claude-3-5-haiku (fallback)
       â”‚
       â””â”€ Standardowe zadanie â”€â”€â†’ ollama/llama3.2:3b (default)
```

### Komendy szybkiego dostÄ™pu

```powershell
# SprawdÅº status providerÃ³w
Get-AIStatus

# Lista lokalnych modeli
Get-LocalModels

# Pojedyncze zapytanie (auto-local)
Invoke-AIRequest -Messages @(@{role="user"; content="Explain X"})

# Batch rÃ³wnolegÅ‚y (auto-local)
Invoke-AIBatch -Prompts @("Task 1", "Task 2", "Task 3")

# WymuÅ› konkretny model
Invoke-AIRequest -Provider "ollama" -Model "qwen2.5-coder:1.5b" -Messages @(...)

# Fallback do cloud gdy local zawiedzie
Invoke-AIRequest -Messages @(...) -AutoFallback
```

### Priorytety kosztowe

| Provider | Model | Koszt/1K tokens | UÅ¼ycie |
|----------|-------|-----------------|--------|
| ollama | * | $0.00 | **DOMYÅšLNY** - zawsze gdy moÅ¼liwe |
| openai | gpt-4o-mini | $0.15/$0.60 | Fallback gdy Ollama niedostÄ™pna |
| anthropic | claude-3-5-haiku | $0.80/$4.00 | ZÅ‚oÅ¼one zadania |
| anthropic | claude-sonnet-4 | $3.00/$15.00 | Tylko krytyczne zadania |

### ReguÅ‚y dla Claude

1. **ZAWSZE sprawdÅº** czy Ollama dziaÅ‚a przed uÅ¼yciem cloud API
2. **PREFERUJ lokalny model** dla prostych zadaÅ„
3. **UÅ»YWAJ parallel** (`Invoke-AIBatch`) gdy masz wiele niezaleÅ¼nych promptÃ³w
4. **FALLBACK do cloud** tylko gdy:
   - Ollama nie dziaÅ‚a
   - Zadanie wymaga duÅ¼ego kontekstu (>32K tokens)
   - JakoÅ›Ä‡ lokalnego modelu niewystarczajÄ…ca
5. **WYBIERZ model specjalistyczny** gdy zadanie pasuje:
   - Kod â†’ `qwen2.5-coder:1.5b`
   - SzybkoÅ›Ä‡ â†’ `llama3.2:1b`
   - OgÃ³lne â†’ `llama3.2:3b`

---

## 11. Advanced AI System (ðŸ§  NEW)

PiÄ™Ä‡ zaawansowanych moduÅ‚Ã³w AI rozszerzajÄ…cych moÅ¼liwoÅ›ci HYDRA:

### Quick Start

```powershell
# Initialize all advanced AI modules
. "C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler\Initialize-AdvancedAI.ps1"

# Run demo
& "C:\Users\BIURODOM\Desktop\ClaudeCLI\ai-handler\Demo-AdvancedAI.ps1"

# Check status
Get-AdvancedAIStatus
```

### 11.1 Agentic Self-Correction

Automatyczna walidacja kodu przez phi3:mini przed prezentacjÄ… uÅ¼ytkownikowi.

| Function | Description |
|----------|-------------|
| `Test-CodeSyntax` | Walidacja skÅ‚adni kodu |
| `Get-CodeLanguage` | Auto-detekcja jÄ™zyka programowania |
| `Invoke-SelfCorrection` | Walidacja z poprawkami |
| `Invoke-CodeWithSelfCorrection` | Generowanie kodu z auto-fix |

```powershell
# Validate code syntax
$result = Test-CodeSyntax -Code "def hello(): print('world')" -Language "python"
# Returns: @{ Valid = $true; Issues = @(); Language = "python" }

# Generate code with automatic validation and retry
$code = Invoke-CodeWithSelfCorrection -Prompt "Write Python factorial" -MaxAttempts 3
# Auto-validates with phi3:mini, regenerates if issues found
```

**Supported Languages**: powershell, python, javascript, typescript, rust, go, sql, csharp, java, html, css

### 11.2 Dynamic Few-Shot Learning

Uczenie siÄ™ z historii udanych odpowiedzi - automatyczne dodawanie przykÅ‚adÃ³w do promptÃ³w.

| Function | Description |
|----------|-------------|
| `Initialize-FewShotCache` | Inicjalizacja cache |
| `Save-SuccessfulResponse` | Zapis udanej odpowiedzi |
| `Get-SuccessfulExamples` | Pobranie podobnych przykÅ‚adÃ³w |
| `Invoke-AIWithFewShot` | Generowanie z przykÅ‚adami |
| `Get-FewShotStats` | Statystyki cache |

```powershell
# Save a successful response for future learning
Save-SuccessfulResponse -Prompt "Write SQL query" -Response "SELECT * FROM users" -Rating 5

# Generate with automatic few-shot examples
$result = Invoke-AIWithFewShot -Prompt "Write SQL to get active users" -Model "llama3.2:3b"
# Automatically includes similar examples from history

# Check cache statistics
Get-FewShotStats
# Returns: TotalEntries, Categories, AverageRating, TotalUses
```

**Categories**: sql, api, code, file, config, docs, test, general

### 11.3 Speculative Decoding

RÃ³wnolegÅ‚e uruchamianie wielu modeli - zwraca najlepszy wynik.

| Function | Description |
|----------|-------------|
| `Invoke-SpeculativeDecoding` | Fast vs Accurate parallel |
| `Invoke-ModelRace` | Race N models, fastest wins |
| `Invoke-CodeSpeculation` | Code-optimized speculation |
| `Invoke-ConsensusGeneration` | Multi-model consensus |
| `Get-TextSimilarity` | PorÃ³wnanie odpowiedzi |

```powershell
# Run fast (1b) and accurate (3b) in parallel
$result = Invoke-SpeculativeDecoding -Prompt "Explain async/await" -TimeoutMs 30000
# Returns best result based on validation

# Race multiple models - fastest valid response wins
$race = Invoke-ModelRace -Prompt "Capital of Japan?" -Models @("llama3.2:1b", "phi3:mini", "llama3.2:3b")
# Winner: llama3.2:1b in 1.76s

# Code with specialized models
$code = Invoke-CodeSpeculation -Prompt "Write JS reverse string" -MaxTokens 512
# Uses llama3.2:1b (fast) + qwen2.5-coder (accurate)

# Multi-model consensus
$consensus = Invoke-ConsensusGeneration -Prompt "Benefits of TypeScript" -Models @("llama3.2:3b", "phi3:mini")
# Returns: Content, Consensus (bool), Similarity (%)
```

### 11.4 Dynamic Load Balancing

Automatyczne przeÅ‚Ä…czanie miÄ™dzy local/cloud na podstawie obciÄ…Å¼enia CPU.

| Function | Description |
|----------|-------------|
| `Get-SystemLoad` | CPU, Memory, Recommendation |
| `Get-CpuLoad` | Quick CPU check |
| `Get-LoadBalancedProvider` | Auto-select provider |
| `Invoke-LoadBalancedBatch` | CPU-aware batch processing |
| `Get-LoadBalancerConfig` | View thresholds |
| `Watch-SystemLoad` | Real-time monitoring |

```powershell
# Check current system load
$load = Get-SystemLoad
# Returns: @{ CpuPercent = 15; MemoryPercent = 45; Recommendation = "local" }

# Auto-select provider based on CPU
$provider = Get-LoadBalancedProvider -Task "code"
# CPU < 70% â†’ ollama (local)
# CPU 70-90% â†’ hybrid
# CPU > 90% â†’ cloud (gpt-4o-mini)

# Batch processing with adaptive load balancing
Invoke-LoadBalancedBatch -Prompts @("Q1", "Q2", "Q3") -AdaptiveBalancing
# Automatically adjusts concurrency based on CPU

# Monitor in real-time
Watch-SystemLoad -IntervalSeconds 2
```

**Thresholds**:
| CPU Load | Recommendation | Provider |
|----------|----------------|----------|
| < 70% | local | ollama |
| 70-90% | hybrid | ollama + cloud |
| > 90% | cloud | openai/anthropic |

### 11.5 Semantic File Mapping

Deep RAG z analizÄ… importÃ³w i zaleÅ¼noÅ›ci - automatyczne rozszerzanie kontekstu.

| Function | Description |
|----------|-------------|
| `Get-FileLanguage` | Detect file language |
| `Get-FileImports` | Extract imports/requires |
| `Get-FileFunctions` | Extract function definitions |
| `Get-RelatedFiles` | Find related by imports |
| `Build-DependencyGraph` | Build project graph |
| `Get-ExpandedContext` | AI context with related files |
| `Invoke-SemanticQuery` | Query with full context |
| `Get-ProjectStructure` | Analyze project structure |

```powershell
# Get related files (follows imports)
$related = Get-RelatedFiles -FilePath "src/app.py" -MaxDepth 2
# Returns files imported by app.py

# Build full dependency graph
$graph = Build-DependencyGraph -ProjectPath "C:\MyProject" -Language "python"
# Returns: nodes (files), edges (dependencies)

# Query with automatic context expansion
$answer = Invoke-SemanticQuery -FilePath "auth.py" -Query "How does login work?" -IncludeRelated
# Automatically includes imported files in AI context

# Get AI context with related files
$context = Get-ExpandedContext -FilePath "main.ts" -MaxRelatedFiles 5
# Returns: MainFile, RelatedFiles, TotalTokens
```

**Supported Languages**: python, javascript, typescript, powershell, rust, go, csharp

### 11.6 Unified Interface

Jeden interfejs Å‚Ä…czÄ…cy wszystkie funkcje z automatycznym wyborem trybu.

| Function | Description |
|----------|-------------|
| `Invoke-AdvancedAI` | Unified generation |
| `Get-OptimalMode` | Auto-detect best mode |
| `New-AICode` | Quick code generation |
| `Get-AIAnalysis` | Analysis with speculation |
| `Get-AIQuick` | Fastest response (racing) |
| `Get-AdvancedAIStatus` | System status |

```powershell
# Unified interface with auto mode selection
Invoke-AdvancedAI -Prompt "Write Python sort function" -Mode auto
# Auto-detects: code â†’ uses self-correction + few-shot

# Available modes
Invoke-AdvancedAI -Prompt "..." -Mode code       # Self-correction + few-shot
Invoke-AdvancedAI -Prompt "..." -Mode analysis   # Speculative decoding
Invoke-AdvancedAI -Prompt "..." -Mode fast       # Model racing
Invoke-AdvancedAI -Prompt "..." -Mode consensus  # Multi-model agreement
Invoke-AdvancedAI -Prompt "..." -Mode fewshot    # Historical examples

# Convenience functions
New-AICode "Python function to download file"    # Code with self-correction
Get-AIAnalysis "Compare REST vs GraphQL"         # Analysis with speculation
Get-AIQuick "What is 2+2?"                       # Fastest response

# Check all modules status
Get-AdvancedAIStatus
```

### Mode Auto-Detection

| Prompt Pattern | Selected Mode | Features Used |
|----------------|---------------|---------------|
| "Write function", "implement", "code" | `code` | Self-correction + Few-shot |
| "Explain", "analyze", "compare" | `analysis` | Speculative decoding |
| "What is", "quick", simple questions | `fast` | Model racing |
| General queries | `fewshot` | Historical examples |

### Performance Benchmarks

| Operation | Time | Models Used |
|-----------|------|-------------|
| Fast mode (racing) | ~2s | llama3.2:1b, phi3:mini |
| Code generation | ~10s | qwen2.5-coder + phi3:mini validation |
| Analysis (speculation) | ~9s | llama3.2:1b + llama3.2:3b parallel |
| Consensus | ~25s | 2-3 models + similarity check |

### Decision Matrix

```
START
  â”‚
  â”œâ”€ Need code? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ New-AICode (self-correction)
  â”‚
  â”œâ”€ Need fastest answer? â”€â”€â”€â”€â”€â”€â†’ Get-AIQuick (model racing)
  â”‚
  â”œâ”€ Need thorough analysis? â”€â”€â”€â†’ Get-AIAnalysis (speculation)
  â”‚
  â”œâ”€ Need multi-model agreement? â†’ Invoke-AdvancedAI -Mode consensus
  â”‚
  â”œâ”€ Have file context? â”€â”€â”€â”€â”€â”€â”€â”€â†’ Invoke-SemanticQuery (deep RAG)
  â”‚
  â””â”€ General query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Invoke-AdvancedAI -Mode auto
```

### 11.7 Prompt Optimizer (ðŸ†• NEW)

Automatyczne ulepszanie promptÃ³w przed wysÅ‚aniem do AI - analiza, kategoryzacja i wzbogacanie.

| Function | Description |
|----------|-------------|
| `Optimize-Prompt` | Main optimizer - analyze & enhance |
| `Get-PromptCategory` | Detect intent (code, analysis, question) |
| `Get-PromptClarity` | Score clarity 0-100 |
| `Get-PromptLanguage` | Detect programming language |
| `Get-BetterPrompt` | Quick one-liner enhancement |
| `Test-PromptQuality` | Visual quality report |
| `Invoke-AIWithOptimization` | AI call with auto-enhancement |

```powershell
# Quick prompt improvement
"explain python" | Get-BetterPrompt
# Returns: "explain python\n\nBe concise but thorough. Provide examples if helpful."

# Full analysis
$result = Optimize-Prompt -Prompt "write code" -Model "llama3.2:3b" -Detailed
# Returns: OptimizedPrompt, Category, ClarityScore, Enhancements

# Test quality
Test-PromptQuality -Prompt "do something with the stuff"
# Shows: Score 45/100, Issues: vague terms, Suggestions: add specifics

# AI call with auto-optimization
Invoke-AIRequest -Messages @(@{role="user"; content="python sort"}) `
    -OptimizePrompt -ShowOptimization
# Auto-enhances prompt before sending
```

**Categories Detected**:

| Category | Triggers | Enhancements Added |
|----------|----------|-------------------|
| `code` | write, implement, function | Clean code, error handling, best practices |
| `analysis` | analyze, compare, explain | Structured analysis, multiple perspectives |
| `question` | what is, how, why, ? | Concise, examples if helpful |
| `creative` | brainstorm, imagine, ideas | Creative, original angles |
| `task` | do, execute, build, setup | Step-by-step, verification |
| `summary` | summarize, tldr, brief | Bullet points, key points only |

**Model-Specific Optimizations**:

| Model Pattern | Style | Prefix Added |
|--------------|-------|--------------|
| `llama3.2:1b` | concise | (none) |
| `qwen2.5-coder` | technical | "You are an expert programmer. " |
| `claude` | detailed | (none) |
| `gpt-4o` | detailed | (none) |

**Auto-Enhancement Rules**:

1. **Category-based**: Adds task-specific instructions
2. **Language tagging**: Prepends `[python]` for detected code languages
3. **Structure wrapper**: Wraps low-clarity prompts (<60 score)
4. **Few-shot injection**: Adds examples from cache (if `-AddExamples`)

```powershell
# Batch optimization (parallel)
$prompts = @("task 1", "task 2", "task 3")
$optimized = Optimize-PromptBatch -Prompts $prompts -Model "llama3.2:3b"
```

---

> *"Trzy gÅ‚owy, jeden cel. Hydra wykonuje rÃ³wnolegle."*
